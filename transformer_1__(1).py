# -*- coding: utf-8 -*-
"""transformer_1_ (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDMCndaRctMK_eSgSsHfbINttqCuvfZj
"""



!pip install gdown

# Replace the link below with your own Drive link
drive_link = "https://drive.google.com/file/d/13HvRNbDMoseYg_TEl1-61kDuy_zaMYNx/view?usp=drive_link"

# Extract the file ID from the link
file_id = drive_link.split('/d/')[1].split('/')[0]

# Use gdown to download the file
!gdown --id {file_id} -O abo_dataset.zip

import os
import sys
import zipfile
import csv
import json
import math
import random
from pathlib import Path
from collections import defaultdict, Counter

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
import nltk

# Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f" Using device: {device}")

# ============================================================
# STEP 1: Dataset Loading & Path Fixing
# ============================================================

def extract_and_explore_dataset(zip_path='abo_dataset.zip', extract_path='./abo_dataset'):
    """Extract dataset and explore actual directory structure"""
    print(f"Step 1: Loading dataset...")

    if not os.path.exists(extract_path):
        print(f" Extracting {zip_path}...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)

    # Explore actual structure
    print(f"Exploring dataset structure...")
    actual_structure = defaultdict(list)

    for root, dirs, files in os.walk(extract_path):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                rel_path = os.path.relpath(os.path.join(root, file), extract_path)
                actual_structure[os.path.dirname(rel_path)].append(file)

    print(f"Found {sum(len(v) for v in actual_structure.values())} images")
    print(f"Directory structure:")
    for dir_name, files in sorted(actual_structure.items()):
        print(f"  {dir_name}: {len(files)} images")

    return extract_path, actual_structure

def load_metadata_and_fix_paths(metadata_path, extract_path, actual_structure):
    """Load metadata and fix image paths based on actual structure"""
    print(f"Loading metadata from {metadata_path}...")

    samples = []
    path_mapping = {}  # Map original paths to actual paths

    # Build reverse mapping of filenames to actual paths
    filename_to_path = {}
    for dir_name, files in actual_structure.items():
        for file in files:
            filename_to_path[file] = os.path.join(extract_path, dir_name, file)

    with open(metadata_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            original_path = row.get('image_path', '')
            caption = row.get('caption', '').strip()
            class_name = row.get('class', 'unknown')

            if not caption:
                continue

            # Extract filename from original path
            filename = os.path.basename(original_path)

            # Find actual path
            if filename in filename_to_path:
                actual_path = filename_to_path[filename]
                if os.path.exists(actual_path):
                    samples.append({
                        'image_path': actual_path,
                        'caption': caption,
                        'class': class_name
                    })
            else:
                # Try to find by partial match
                for fname, fpath in filename_to_path.items():
                    if filename in fname or fname in filename:
                        if os.path.exists(fpath):
                            samples.append({
                                'image_path': fpath,
                                'caption': caption,
                                'class': class_name
                            })
                            break

    print(f"Loaded {len(samples)} valid samples with existing images")

    if len(samples) == 0:
        print(" WARNING: No valid samples found! Check dataset structure.")

    return samples

# ============================================================
# STEP 2: Tokenizer
# ============================================================

class Tokenizer:
    def __init__(self, vocab_size=5000):
        self.vocab_size = vocab_size
        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}
        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}
        self.word_freq = Counter()

    def build_vocab(self, captions):
        """Build vocabulary from captions"""
        print(f"Building vocabulary...")

        for caption in captions:
            words = caption.lower().split()
            self.word_freq.update(words)

        # Add top words to vocab
        idx = 4
        for word, freq in self.word_freq.most_common(self.vocab_size - 4):
            self.word2idx[word] = idx
            self.idx2word[idx] = word
            idx += 1

        print(f"Vocabulary size: {len(self.word2idx)}")

    def encode(self, caption, max_len=50):
        """Encode caption to indices"""
        tokens = [self.word2idx['<SOS>']]
        words = caption.lower().split()

        for word in words:
            if len(tokens) >= max_len:
                break
            tokens.append(self.word2idx.get(word, self.word2idx['<UNK>']))

        tokens.append(self.word2idx['<EOS>'])
        return tokens

    def decode(self, indices):
        """Decode indices to caption"""
        words = []
        for idx in indices:
            if idx == self.word2idx['<EOS>']:
                break
            if idx != self.word2idx['<SOS>'] and idx != self.word2idx['<PAD>']:
                words.append(self.idx2word.get(idx, '<UNK>'))
        return ' '.join(words)

# ============================================================
# STEP 3: Dataset Class
# ============================================================

class ImageCaptionDataset(Dataset):
    def __init__(self, samples, tokenizer, max_caption_len=50, img_size=224):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_caption_len = max_caption_len
        self.img_size = img_size

        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        try:
            image = Image.open(sample['image_path']).convert('RGB')
            image = self.transform(image)
        except Exception as e:
            print(f"Error loading {sample['image_path']}: {e}")
            # Return a blank image on error
            image = torch.zeros(3, self.img_size, self.img_size)

        caption_tokens = self.tokenizer.encode(sample['caption'], self.max_caption_len)
        caption = torch.tensor(caption_tokens, dtype=torch.long)

        return image, caption

def collate_fn(batch):
    """Collate function for DataLoader"""
    images, captions = zip(*batch)
    images = torch.stack(images)
    captions = pad_sequence(captions, batch_first=True, padding_value=0)
    return images, captions

# ============================================================
# STEP 4: Model Architecture
# ============================================================

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class CNNEncoder(nn.Module):
    """Custom CNN encoder trained from scratch"""
    def __init__(self, embed_dim=512):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.layer1 = self._make_layer(64, 64, 3, stride=1)
        self.layer2 = self._make_layer(64, 128, 4, stride=2)
        self.layer3 = self._make_layer(128, 256, 6, stride=2)
        self.layer4 = self._make_layer(256, 512, 3, stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, embed_dim)

    def _make_layer(self, in_channels, out_channels, blocks, stride):
        layers = []
        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride, 1))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))

        for _ in range(1, blocks):
            layers.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(nn.ReLU(inplace=True))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

class TransformerDecoder(nn.Module):
    """Transformer decoder for caption generation"""
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=4, dim_feedforward=2048, dropout=0.2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)

        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            batch_first=True,
            dropout=dropout  # increased dropout to 0.2 for better regularization
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, captions, encoder_output, tgt_mask=None):
        """
        captions: (batch_size, seq_len)
        encoder_output: (batch_size, d_model)
        """
        x = self.embedding(captions)
        x = self.pos_encoding(x)

        # Expand encoder output to match sequence length
        memory = encoder_output.unsqueeze(1).expand(-1, x.size(1), -1)

        x = self.transformer_decoder(x, memory, tgt_mask=tgt_mask)
        logits = self.fc_out(x)
        return logits

class ImageCaptioningModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=512, nhead=8, num_layers=4, dropout=0.2):
        super().__init__()
        self.encoder = CNNEncoder(embed_dim)
        self.decoder = TransformerDecoder(vocab_size, embed_dim, nhead, num_layers, dropout=dropout)

    def forward(self, images, captions, tgt_mask=None):
        encoder_output = self.encoder(images)
        logits = self.decoder(captions, encoder_output, tgt_mask)
        return logits

# ============================================================
# STEP 5: Training Loop with Early Stopping
# ============================================================

def create_causal_mask(seq_len, device):
    """Create causal mask for autoregressive generation"""
    mask = torch.triu(torch.ones(seq_len, seq_len, device=device) * float('-inf'), diagonal=1)
    return mask

def train_epoch(model, train_loader, optimizer, criterion, device, tokenizer):
    model.train()
    total_loss = 0
    nan_count = 0

    for batch_idx, (images, captions) in enumerate(train_loader):
        images = images.to(device)
        captions = captions.to(device)

        # Remove EOS token from input, keep SOS
        input_captions = captions[:, :-1]
        target_captions = captions[:, 1:]

        # Create causal mask
        seq_len = input_captions.size(1)
        tgt_mask = create_causal_mask(seq_len, device)

        optimizer.zero_grad()

        logits = model(images, input_captions, tgt_mask=tgt_mask)
        loss = criterion(logits.reshape(-1, logits.size(-1)), target_captions.reshape(-1))

        if torch.isnan(loss) or torch.isinf(loss):
            print(f"  WARNING: NaN/Inf loss detected at batch {batch_idx + 1}, skipping...")
            nan_count += 1
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

        if (batch_idx + 1) % 100 == 0:
            print(f"Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}")

    return total_loss / max(len(train_loader) - nan_count, 1)

def validate(model, val_loader, criterion, device, tokenizer):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for images, captions in val_loader:
            images = images.to(device)
            captions = captions.to(device)

            input_captions = captions[:, :-1]
            target_captions = captions[:, 1:]

            seq_len = input_captions.size(1)
            tgt_mask = create_causal_mask(seq_len, device)

            logits = model(images, input_captions, tgt_mask=tgt_mask)
            loss = criterion(logits.reshape(-1, logits.size(-1)), target_captions.reshape(-1))
            total_loss += loss.item()

    return total_loss / len(val_loader)

def generate_caption(model, image, tokenizer, device, max_len=50):
    """Generate caption for a single image"""
    model.eval()
    with torch.no_grad():
        image = image.unsqueeze(0).to(device)
        encoder_output = model.encoder(image)

        caption_tokens = [tokenizer.word2idx['<SOS>']]

        for _ in range(max_len):
            input_tokens = torch.tensor([caption_tokens], dtype=torch.long).to(device)

            seq_len = input_tokens.size(1)
            tgt_mask = create_causal_mask(seq_len, device)

            logits = model.decoder(input_tokens, encoder_output, tgt_mask=tgt_mask)
            next_token = logits[0, -1, :].argmax().item()

            caption_tokens.append(next_token)

            if next_token == tokenizer.word2idx['<EOS>']:
                break

        return tokenizer.decode(caption_tokens)

# ============================================================
# MAIN EXECUTION
# ============================================================

if __name__ == '__main__':
    print("Starting Image Captioning Training Pipeline (350 Epochs - Regularized)...\n")

    # Step 1: Load and fix dataset
    extract_path, actual_structure = extract_and_explore_dataset()
    metadata_path = os.path.join(extract_path, 'metadata.csv')

    if not os.path.exists(metadata_path):
        print("ERROR: metadata.csv not found!")
        sys.exit(1)

    samples = load_metadata_and_fix_paths(metadata_path, extract_path, actual_structure)
    samples = load_metadata_and_fix_paths(metadata_path, extract_path, actual_structure)

    # ✅ Filter to only mugs and bottles
    samples = [s for s in samples if s['class'].lower() in ['mugs', 'bottles']]
    print(f"Filtered dataset: {len(samples)} samples (only mugs and bottles)")

    if len(samples) == 0:
        print("ERROR: No samples found for mugs and bottles!")
        sys.exit(1)

    # Step 2: Build tokenizer
    print(f"\nStep 2: Building tokenizer...")
    tokenizer = Tokenizer(vocab_size=5000)
    captions = [s['caption'] for s in samples]
    tokenizer.build_vocab(captions)

    # Step 3: Create dataloaders
    print(f"\nStep 3: Creating dataloaders...")
    random.shuffle(samples)
    split_idx = int(0.8 * len(samples))
    train_samples = samples[:split_idx]
    val_samples = samples[split_idx:]

    train_dataset = ImageCaptionDataset(train_samples, tokenizer)
    val_dataset = ImageCaptionDataset(val_samples, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2)

    print(f"Train samples: {len(train_samples)}, Val samples: {len(val_samples)}")

    # Step 4: Initialize model
    print(f"\nStep 4: Initializing model...")
    model = ImageCaptioningModel(vocab_size=len(tokenizer.word2idx), embed_dim=512, nhead=8, num_layers=4, dropout=0.2)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    model = model.to(device)

    # Step 5: Setup training
    print(f"\n Step 5: Setting up training...")
    optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=350)

    best_val_loss = float('inf')
    patience = 30  # Stop if no improvement for 30 epochs
    patience_counter = 0

    # Step 6: Training loop
    print(f"\n Step 6: Starting training...\n")
    train_losses = []
    val_losses = []

    for epoch in range(350):
        print("=" * 60)
        print(f"Epoch {epoch + 1}/350")
        print("=" * 60)

        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, tokenizer)
        val_loss = validate(model, val_loader, criterion, device, tokenizer)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        gap = val_loss - train_loss
        print(f" Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Gap: {gap:.4f}")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\n")

        scheduler.step()

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'model_best.pt')
            print(f" ✓ Best model saved! Val Loss: {val_loss:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\n EARLY STOPPING: No improvement for {patience} epochs")
                print(f" Best validation loss: {best_val_loss:.4f}")
                break

        # Save checkpoint every 10 epochs
        if (epoch + 1) % 10 == 0:
            torch.save(model.state_dict(), f'model_epoch_{epoch + 1}.pt')
            print(f" Saved checkpoint: model_epoch_{epoch + 1}.pt\n")

    # Step 7: Load best model for evaluation
    print("\n Loading best model for evaluation...")
    model.load_state_dict(torch.load('model_best.pt'))

    # Step 8: Evaluation
    print("\n" + "=" * 60)
    print("Evaluation & Sample Predictions")
    print("=" * 60)

    # Plot training curves
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training Curves (350 Epochs - Regularized)')
    plt.savefig('training_curves_regularized.png', dpi=100, bbox_inches='tight')
    print(" Saved training curves to training_curves_regularized.png")

    # Generate sample captions
    print("\n Generating sample captions...\n")
    model.eval()

    sample_indices = random.sample(range(len(val_dataset)), min(5, len(val_dataset)))

    for idx in sample_indices:
        image, _ = val_dataset[idx]
        generated_caption = generate_caption(model, image, tokenizer, device)
        actual_caption = val_samples[idx]['caption']

        print(f"Actual:    {actual_caption}")
        print(f"Generated: {generated_caption}")
        print()

    print(" Training complete!")

from google.colab import files

files.download('model_epoch_70.pt')  # replace with your filename

import os
import sys
import zipfile
import csv
import json
import random
from pathlib import Path
from collections import defaultdict, Counter

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
import nltk

# Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"[v0] Using device: {device}")

# ============================================================
# STEP 1: Dataset Loading & Path Fixing (SAME AS TRAINING)
# ============================================================

def extract_and_explore_dataset(zip_path='abo_dataset.zip', extract_path='./abo_dataset'):
    """Extract dataset and explore actual directory structure"""
    print(f"[v0] Step 1: Loading dataset...")

    if not os.path.exists(extract_path):
        print(f"[v0] Extracting {zip_path}...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)

    # Explore actual structure
    print(f"[v0] Exploring dataset structure...")
    actual_structure = defaultdict(list)

    for root, dirs, files in os.walk(extract_path):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                rel_path = os.path.relpath(os.path.join(root, file), extract_path)
                actual_structure[os.path.dirname(rel_path)].append(file)

    print(f"[v0] Found {sum(len(v) for v in actual_structure.values())} images")
    print(f"[v0] Directory structure:")
    for dir_name, files in sorted(actual_structure.items()):
        print(f"  {dir_name}: {len(files)} images")

    return extract_path, actual_structure

def load_metadata_and_fix_paths(metadata_path, extract_path, actual_structure):
    """Load metadata and fix image paths based on actual structure"""
    print(f"[v0] Loading metadata from {metadata_path}...")

    samples = []
    path_mapping = {}

    # Build reverse mapping of filenames to actual paths
    filename_to_path = {}
    for dir_name, files in actual_structure.items():
        for file in files:
            filename_to_path[file] = os.path.join(extract_path, dir_name, file)

    with open(metadata_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            original_path = row.get('image_path', '')
            caption = row.get('caption', '').strip()
            class_name = row.get('class', 'unknown')

            if not caption:
                continue

            # Extract filename from original path
            filename = os.path.basename(original_path)

            # Find actual path
            if filename in filename_to_path:
                actual_path = filename_to_path[filename]
                if os.path.exists(actual_path):
                    samples.append({
                        'image_path': actual_path,
                        'caption': caption,
                        'class': class_name
                    })
            else:
                # Try to find by partial match
                for fname, fpath in filename_to_path.items():
                    if filename in fname or fname in filename:
                        if os.path.exists(fpath):
                            samples.append({
                                'image_path': fpath,
                                'caption': caption,
                                'class': class_name
                            })
                            break

    print(f"[v0] Loaded {len(samples)} valid samples with existing images")

    if len(samples) == 0:
        print("[v0] WARNING: No valid samples found! Check dataset structure.")

    return samples

# ============================================================
# STEP 2: Tokenizer & Model Components
# ============================================================

class Tokenizer:
    def __init__(self, vocab_size=5000):
        self.vocab_size = vocab_size
        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}
        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}
        self.word_freq = Counter()

    def build_vocab(self, captions):
        print(f"[v0] Building vocabulary...")
        for caption in captions:
            words = caption.lower().split()
            self.word_freq.update(words)

        idx = 4
        for word, freq in self.word_freq.most_common(self.vocab_size - 4):
            self.word2idx[word] = idx
            self.idx2word[idx] = word
            idx += 1

        print(f"[v0] Vocabulary size: {len(self.word2idx)}")

    def encode(self, caption, max_len=50):
        tokens = [self.word2idx['<SOS>']]
        words = caption.lower().split()

        for word in words:
            if len(tokens) >= max_len:
                break
            tokens.append(self.word2idx.get(word, self.word2idx['<UNK>']))

        tokens.append(self.word2idx['<EOS>'])
        return tokens

    def decode(self, indices):
        words = []
        for idx in indices:
            if idx == self.word2idx['<EOS>']:
                break
            if idx != self.word2idx['<SOS>'] and idx != self.word2idx['<PAD>']:
                words.append(self.idx2word.get(idx, '<UNK>'))
        return ' '.join(words)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class CNNEncoder(nn.Module):
    def __init__(self, embed_dim=512):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.layer1 = self._make_layer(64, 64, 3, stride=1)
        self.layer2 = self._make_layer(64, 128, 4, stride=2)
        self.layer3 = self._make_layer(128, 256, 6, stride=2)
        self.layer4 = self._make_layer(256, 512, 3, stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, embed_dim)

    def _make_layer(self, in_channels, out_channels, blocks, stride):
        layers = []
        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride, 1))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))

        for _ in range(1, blocks):
            layers.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(nn.ReLU(inplace=True))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=4, dim_feedforward=2048):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)

        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            batch_first=True,
            dropout=0.1
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, captions, encoder_output, tgt_mask=None):
        x = self.embedding(captions)
        x = self.pos_encoding(x)
        memory = encoder_output.unsqueeze(1).expand(-1, x.size(1), -1)
        x = self.transformer_decoder(x, memory, tgt_mask=tgt_mask)
        logits = self.fc_out(x)
        return logits

class ImageCaptioningModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=512, nhead=8, num_layers=4):
        super().__init__()
        self.encoder = CNNEncoder(embed_dim)
        self.decoder = TransformerDecoder(vocab_size, embed_dim, nhead, num_layers)

    def forward(self, images, captions, tgt_mask=None):
        encoder_output = self.encoder(images)
        logits = self.decoder(captions, encoder_output, tgt_mask)
        return logits

# ============================================================
# STEP 3: Dataset & Utilities
# ============================================================

class ImageCaptionDataset(Dataset):
    def __init__(self, samples, tokenizer, max_caption_len=50, img_size=224):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_caption_len = max_caption_len
        self.img_size = img_size

        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        try:
            image = Image.open(sample['image_path']).convert('RGB')
            image = self.transform(image)
        except Exception as e:
            print(f"[v0] Error loading {sample['image_path']}: {e}")
            image = torch.zeros(3, self.img_size, self.img_size)

        caption_tokens = self.tokenizer.encode(sample['caption'], self.max_caption_len)
        caption = torch.tensor(caption_tokens, dtype=torch.long)

        return image, caption, sample['caption'], sample['image_path']

def collate_fn(batch):
    images, captions, actual_captions, image_paths = zip(*batch)
    images = torch.stack(images)
    captions = pad_sequence(captions, batch_first=True, padding_value=0)
    return images, captions, actual_captions, image_paths

def create_causal_mask(seq_len, device):
    mask = torch.triu(torch.ones(seq_len, seq_len, device=device) * float('-inf'), diagonal=1)
    return mask

def generate_caption(model, image, tokenizer, device, max_len=50):
    model.eval()
    with torch.no_grad():
        image = image.unsqueeze(0).to(device)
        encoder_output = model.encoder(image)

        caption_tokens = [tokenizer.word2idx['<SOS>']]

        for _ in range(max_len):
            input_tokens = torch.tensor([caption_tokens], dtype=torch.long).to(device)
            seq_len = input_tokens.size(1)
            tgt_mask = create_causal_mask(seq_len, device)

            logits = model.decoder(input_tokens, encoder_output, tgt_mask=tgt_mask)
            next_token = logits[0, -1, :].argmax().item()

            caption_tokens.append(next_token)

            if next_token == tokenizer.word2idx['<EOS>']:
                break

        return tokenizer.decode(caption_tokens)

# ============================================================
# STEP 4: Metrics Calculation
# ============================================================

def calculate_bleu_score(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25)):
    """Calculate BLEU score for a single caption"""
    ref_tokens = reference.lower().split()
    hyp_tokens = hypothesis.lower().split()

    if len(hyp_tokens) == 0:
        return 0.0

    smoothing_function = SmoothingFunction().method1
    score = sentence_bleu([ref_tokens], hyp_tokens, weights=weights, smoothing_function=smoothing_function)
    return score

def calculate_metrics(actual_captions, generated_captions):
    """Calculate comprehensive metrics"""
    bleu_scores = []
    caption_lengths_actual = []
    caption_lengths_generated = []
    word_overlap_ratios = []

    for actual, generated in zip(actual_captions, generated_captions):
        # BLEU Score
        bleu = calculate_bleu_score(actual, generated)
        bleu_scores.append(bleu)

        # Caption lengths
        actual_len = len(actual.lower().split())
        generated_len = len(generated.lower().split())
        caption_lengths_actual.append(actual_len)
        caption_lengths_generated.append(generated_len)

        # Word overlap ratio
        actual_words = set(actual.lower().split())
        generated_words = set(generated.lower().split())
        if len(actual_words.union(generated_words)) > 0:
            overlap = len(actual_words.intersection(generated_words)) / len(actual_words.union(generated_words))
            word_overlap_ratios.append(overlap)

    metrics = {
        'bleu_scores': bleu_scores,
        'avg_bleu': np.mean(bleu_scores),
        'std_bleu': np.std(bleu_scores),
        'caption_lengths_actual': caption_lengths_actual,
        'caption_lengths_generated': caption_lengths_generated,
        'avg_length_actual': np.mean(caption_lengths_actual),
        'avg_length_generated': np.mean(caption_lengths_generated),
        'word_overlap_ratios': word_overlap_ratios,
        'avg_word_overlap': np.mean(word_overlap_ratios),
    }

    return metrics

# ============================================================
# STEP 5: Visualization Functions
# ============================================================

def plot_sample_predictions(test_dataset, model, tokenizer, device, num_samples=6):
    """Plot sample predictions with images"""
    print(f"[v0] Creating sample predictions visualization...")

    sample_indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))

    fig = plt.figure(figsize=(16, 12))
    gs = gridspec.GridSpec(3, 2, figure=fig, hspace=0.4, wspace=0.3)

    for idx, sample_idx in enumerate(sample_indices):
        ax = fig.add_subplot(gs[idx])

        image, _, actual_caption, image_path = test_dataset[sample_idx]
        generated_caption = generate_caption(model, image, tokenizer, device)

        # Load and display image
        try:
            img = Image.open(image_path).convert('RGB')
            ax.imshow(img)
        except:
            ax.text(0.5, 0.5, 'Image not found', ha='center', va='center')

        ax.axis('off')

        # Add text
        title = f"Actual: {actual_caption[:50]}...\n\nGenerated: {generated_caption[:50]}..."
        ax.set_title(title, fontsize=10, pad=10, wrap=True)

    plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')
    print("[v0] Saved sample predictions to sample_predictions.png")
    plt.close()

def plot_metrics_distribution(metrics):
    """Plot distribution of metrics"""
    print(f"[v0] Creating metrics distribution plots...")

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # BLEU Score Distribution
    axes[0, 0].hist(metrics['bleu_scores'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(metrics['avg_bleu'], color='red', linestyle='--', linewidth=2, label=f"Mean: {metrics['avg_bleu']:.4f}")
    axes[0, 0].set_xlabel('BLEU Score')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('BLEU Score Distribution')
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)

    # Caption Length Comparison
    axes[0, 1].scatter(metrics['caption_lengths_actual'], metrics['caption_lengths_generated'], alpha=0.6, s=50)
    max_len = max(max(metrics['caption_lengths_actual']), max(metrics['caption_lengths_generated']))
    axes[0, 1].plot([0, max_len], [0, max_len], 'r--', linewidth=2, label='Perfect Match')
    axes[0, 1].set_xlabel('Actual Caption Length')
    axes[0, 1].set_ylabel('Generated Caption Length')
    axes[0, 1].set_title('Caption Length Comparison')
    axes[0, 1].legend()
    axes[0, 1].grid(alpha=0.3)

    # Word Overlap Distribution
    axes[1, 0].hist(metrics['word_overlap_ratios'], bins=30, color='lightgreen', edgecolor='black', alpha=0.7)
    axes[1, 0].axvline(metrics['avg_word_overlap'], color='red', linestyle='--', linewidth=2, label=f"Mean: {metrics['avg_word_overlap']:.4f}")
    axes[1, 0].set_xlabel('Word Overlap Ratio')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Word Overlap Distribution')
    axes[1, 0].legend()
    axes[1, 0].grid(alpha=0.3)

    # Length Difference Distribution
    length_diffs = [abs(a - g) for a, g in zip(metrics['caption_lengths_actual'], metrics['caption_lengths_generated'])]
    axes[1, 1].hist(length_diffs, bins=30, color='salmon', edgecolor='black', alpha=0.7)
    axes[1, 1].set_xlabel('Absolute Length Difference')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].set_title('Caption Length Difference Distribution')
    axes[1, 1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('metrics_distribution.png', dpi=150, bbox_inches='tight')
    print("[v0] Saved metrics distribution to metrics_distribution.png")
    plt.close()

def plot_detailed_statistics(metrics):
    """Plot detailed statistics"""
    print(f"[v0] Creating detailed statistics plots...")

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # BLEU Score by Quartile
    bleu_sorted = sorted(metrics['bleu_scores'])
    quartiles = [bleu_sorted[i*len(bleu_sorted)//4:(i+1)*len(bleu_sorted)//4] for i in range(4)]
    quartile_means = [np.mean(q) if q else 0 for q in quartiles]
    axes[0, 0].bar(['Q1', 'Q2', 'Q3', 'Q4'], quartile_means, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
    axes[0, 0].set_ylabel('Average BLEU Score')
    axes[0, 0].set_title('BLEU Score by Quartile')
    axes[0, 0].grid(alpha=0.3, axis='y')

    # Caption Length Statistics
    categories = ['Actual', 'Generated']
    lengths = [metrics['avg_length_actual'], metrics['avg_length_generated']]
    axes[0, 1].bar(categories, lengths, color=['#3498db', '#e74c3c'], alpha=0.7)
    axes[0, 1].set_ylabel('Average Length (words)')
    axes[0, 1].set_title('Average Caption Length Comparison')
    axes[0, 1].grid(alpha=0.3, axis='y')

    # Metrics Summary Table
    axes[1, 0].axis('off')
    summary_data = [
        ['Metric', 'Value'],
        ['Avg BLEU Score', f"{metrics['avg_bleu']:.4f}"],
        ['Std BLEU Score', f"{metrics['std_bleu']:.4f}"],
        ['Avg Word Overlap', f"{metrics['avg_word_overlap']:.4f}"],
        ['Avg Actual Length', f"{metrics['avg_length_actual']:.2f}"],
        ['Avg Generated Length', f"{metrics['avg_length_generated']:.2f}"],
        ['Total Samples', f"{len(metrics['bleu_scores'])}"],
    ]

    table = axes[1, 0].table(cellText=summary_data, cellLoc='center', loc='center',
                             colWidths=[0.5, 0.5])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)

    # Header styling
    for i in range(2):
        table[(0, i)].set_facecolor('#34495e')
        table[(0, i)].set_text_props(weight='bold', color='white')

    axes[1, 0].set_title('Evaluation Metrics Summary', fontsize=12, weight='bold', pad=20)

    # BLEU Score Percentiles
    percentiles = [10, 25, 50, 75, 90]
    percentile_values = [np.percentile(metrics['bleu_scores'], p) for p in percentiles]
    axes[1, 1].plot(percentiles, percentile_values, marker='o', linewidth=2, markersize=8, color='#9b59b6')
    axes[1, 1].fill_between(percentiles, percentile_values, alpha=0.3, color='#9b59b6')
    axes[1, 1].set_xlabel('Percentile')
    axes[1, 1].set_ylabel('BLEU Score')
    axes[1, 1].set_title('BLEU Score Percentiles')
    axes[1, 1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('detailed_statistics.png', dpi=150, bbox_inches='tight')
    print("[v0] Saved detailed statistics to detailed_statistics.png")
    plt.close()

# ============================================================
# STEP 6: Main Evaluation Pipeline
# ============================================================

if __name__ == '__main__':
    print("[v0] Starting Image Captioning Evaluation Pipeline...\n")

    extract_path, actual_structure = extract_and_explore_dataset()
    metadata_path = os.path.join(extract_path, 'metadata.csv')

    if not os.path.exists(metadata_path):
        print("[v0] ERROR: metadata.csv not found!")
        sys.exit(1)



    samples = load_metadata_and_fix_paths(metadata_path, extract_path, actual_structure)

    # ✅ Filter to only mugs and bottles
    samples = [s for s in samples if s['class'].lower() in ['mugs', 'bottles']]
    print(f"Filtered dataset: {len(samples)} samples (only mugs and bottles)")

    if len(samples) == 0:
        print("ERROR: No samples found for mugs and bottles!")
        sys.exit(1)


    # Build tokenizer
    print("\n[v0] Building tokenizer...")
    tokenizer = Tokenizer(vocab_size=5000)
    captions = [s['caption'] for s in samples]
    tokenizer.build_vocab(captions)

    # Split into test set
    random.shuffle(samples)
    test_samples = samples[:int(0.2 * len(samples))]

    print(f"[v0] Test samples: {len(test_samples)}")

    # Create dataset
    test_dataset = ImageCaptionDataset(test_samples, tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)

    # Load model
    print("[v0] Loading trained model...")
    model = ImageCaptioningModel(vocab_size=len(tokenizer.word2idx), embed_dim=512, nhead=8, num_layers=4)

    # Try to load checkpoint
    checkpoint_path = 'model_epoch_70.pt'
    if os.path.exists(checkpoint_path):
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        print(f"[v0] Loaded checkpoint: {checkpoint_path}")
    else:
        print(f"[v0] WARNING: Checkpoint not found at {checkpoint_path}")
        print("[v0] Using untrained model for demonstration")

    model = model.to(device)
    model.eval()

    # Generate predictions
    print("\n[v0] Generating predictions on test set...")
    all_actual_captions = []
    all_generated_captions = []

    with torch.no_grad():
        for batch_idx, (images, captions, actual_captions, image_paths) in enumerate(test_loader):
            for i in range(len(images)):
                image = images[i:i+1]
                generated_caption = generate_caption(model, image.squeeze(0), tokenizer, device)
                all_actual_captions.append(actual_captions[i])
                all_generated_captions.append(generated_caption)

            if (batch_idx + 1) % 50 == 0:
                print(f"[v0] Processed {batch_idx + 1}/{len(test_loader)} batches")

    print(f"[v0] Generated {len(all_generated_captions)} captions")

    # Calculate metrics
    print("\n[v0] Calculating metrics...")
    metrics = calculate_metrics(all_actual_captions, all_generated_captions)

    # Print detailed results
    print("\n" + "="*60)
    print("EVALUATION RESULTS")
    print("="*60)
    print(f"Total Test Samples: {len(all_actual_captions)}")
    print(f"\nBLEU Score Metrics:")
    print(f"  Average BLEU: {metrics['avg_bleu']:.4f}")
    print(f"  Std Dev BLEU: {metrics['std_bleu']:.4f}")
    print(f"  Min BLEU: {min(metrics['bleu_scores']):.4f}")
    print(f"  Max BLEU: {max(metrics['bleu_scores']):.4f}")
    print(f"\nCaption Length Metrics:")
    print(f"  Avg Actual Length: {metrics['avg_length_actual']:.2f} words")
    print(f"  Avg Generated Length: {metrics['avg_length_generated']:.2f} words")
    print(f"\nWord Overlap Metrics:")
    print(f"  Average Word Overlap: {metrics['avg_word_overlap']:.4f}")
    print("="*60)

    # Save detailed results to JSON
    results = {
        'total_samples': len(all_actual_captions),
        'metrics': {
            'avg_bleu': float(metrics['avg_bleu']),
            'std_bleu': float(metrics['std_bleu']),
            'min_bleu': float(min(metrics['bleu_scores'])),
            'max_bleu': float(max(metrics['bleu_scores'])),
            'avg_length_actual': float(metrics['avg_length_actual']),
            'avg_length_generated': float(metrics['avg_length_generated']),
            'avg_word_overlap': float(metrics['avg_word_overlap']),
        },
        'sample_predictions': [
            {
                'actual': actual,
                'generated': generated,
                'bleu_score': float(bleu)
            }
            for actual, generated, bleu in zip(all_actual_captions[:20], all_generated_captions[:20], metrics['bleu_scores'][:20])
        ]
    }

    with open('evaluation_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print("\n[v0] Saved detailed results to evaluation_results.json")

    # Generate visualizations
    print("\n[v0] Generating visualizations...")
    plot_sample_predictions(test_dataset, model, tokenizer, device, num_samples=6)
    plot_metrics_distribution(metrics)
    plot_detailed_statistics(metrics)

    print("\n[v0] Evaluation complete! Check the generated PNG files for visualizations.")

from IPython.display import Image, display

# Example: show an image from /content/
display(Image(filename='/content/sample_predictions.png'))
display(Image(filename='/content/detailed_statistics.png'))
display(Image(filename='/content/metrics_distribution.png'))

import pickle

# Assuming you already have your `tokenizer` object from your training code
tokenizer_path = "tokenizer.pkl"

with open(tokenizer_path, "wb") as f:
    pickle.dump(tokenizer, f)

print(f"[v0] Tokenizer saved successfully at {tokenizer_path}")
print(f"Vocab size: {len(tokenizer.word2idx)}")

